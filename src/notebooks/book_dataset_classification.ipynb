{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad37a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6c67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "remove_underscore = True\n",
    "\n",
    "def _norm(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    return s.replace(\"_\", \" \") if remove_underscore else s\n",
    "\n",
    "def _new_unknown(unk_list):\n",
    "    u = f\"unknown_{len(unk_list)}\"\n",
    "    unk_list.append(u)\n",
    "    return u\n",
    "\n",
    "def generate(claim: str, sample: Dict[str, Any]):\n",
    "    # Normalize entities by adding the underscore\n",
    "    entity_set = [_norm(e) for e in sample.get(\"Entity_set\", [])]\n",
    "\n",
    "    # Get evidences\n",
    "    evidence = sample.get(\"Evidence\", {})\n",
    "\n",
    "    # Normalize keys\n",
    "    evidence_map = { _norm(k): v for k, v in evidence.items() }\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 1 — Build direct matches (A --rel--> B)\n",
    "    # --------------------------------------------------------------\n",
    "    direct_edges = []\n",
    "\n",
    "    for e1, groups1 in evidence_map.items():\n",
    "        for rels1 in groups1:\n",
    "            for r in rels1:\n",
    "                inverse = r.startswith(\"~\")\n",
    "                rel = r[1:] if inverse else r\n",
    "\n",
    "                # CASE 1: forward r → look for entity with ~r\n",
    "                if not inverse:\n",
    "                    for e2, groups2 in evidence_map.items():\n",
    "                        if e1 == e2: continue\n",
    "                        for rels2 in groups2:\n",
    "                            if f\"~{rel}\" in rels2:\n",
    "                                direct_edges.append((e1, rel, e2))\n",
    "\n",
    "                # CASE 2: inverse ~r → look for entity with forward r\n",
    "                else:\n",
    "                    for e2, groups2 in evidence_map.items():\n",
    "                        if e1 == e2: continue\n",
    "                        for rels2 in groups2:\n",
    "                            if rel in rels2:\n",
    "                                direct_edges.append((e2, rel, e1))\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 2 — Now build multi-hop paths (using unknown_i)\n",
    "    # --------------------------------------------------------------\n",
    "    unknown_list = []\n",
    "    triplets = []\n",
    "\n",
    "    # dictionary to quickly check for direct pairs\n",
    "    direct_set = set((h, r, t) for h, r, t in direct_edges)\n",
    "\n",
    "    # Helper to detect if last hop of unit should link directly\n",
    "    def find_direct_target(rel, exclude):\n",
    "        for h, r, t in direct_edges:\n",
    "            if r == rel:\n",
    "                if h != exclude:\n",
    "                    return t\n",
    "        return None\n",
    "\n",
    "    for ent, rel_groups in evidence_map.items():\n",
    "        for rel_path in rel_groups:\n",
    "\n",
    "            # If path has only 1 rel, and direct match exists → already captured\n",
    "            if len(rel_path) == 1:\n",
    "                r = rel_path[0]\n",
    "                inv = r.startswith(\"~\")\n",
    "                rel = r[1:] if inv else r\n",
    "\n",
    "                # direct already handled → skip\n",
    "                continue\n",
    "\n",
    "            # Multi-hop path → must use unknowns\n",
    "            curr = ent\n",
    "            for i, r in enumerate(rel_path):\n",
    "                inv = r.startswith(\"~\")\n",
    "                rel = r[1:] if inv else r\n",
    "\n",
    "                # last hop? try direct match\n",
    "                if i == len(rel_path) - 1:\n",
    "                    target = find_direct_target(rel, curr)\n",
    "                    if target is None:\n",
    "                        target = _new_unknown(unknown_list)\n",
    "                else:\n",
    "                    # internal hop → always unknown\n",
    "                    target = _new_unknown(unknown_list)\n",
    "\n",
    "                # build triplet\n",
    "                if inv:\n",
    "                    triplets.append((target, rel, curr))\n",
    "                else:\n",
    "                    triplets.append((curr, rel, target))\n",
    "\n",
    "                curr = target\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 3 — Combine direct + multi-hop, deduplicate\n",
    "    # --------------------------------------------------------------\n",
    "    triplets.extend(direct_edges)\n",
    "    final = list(dict.fromkeys(triplets))\n",
    "\n",
    "    sample[\"triplet\"] = final\n",
    "    return sample\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "def linearize(triplets: List[Tuple[str, str, str]]) -> str:\n",
    "    return \"\\n\".join(f\"<e>{h}</e> || {r} || <e>{t}</e>\" for h, r, t in triplets)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(data: dict, remove_underscore: bool = True) -> Tuple[Dict, List]:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    updated_data = {}\n",
    "    distinct_entities = set()\n",
    "    keys = list(data.keys())\n",
    "\n",
    "    for key in tqdm(keys, desc=\"Processing data\"):\n",
    "        updated = generate(key, data[key])\n",
    "        updated_data[key] = updated\n",
    "\n",
    "        for h, r, t in updated[\"triplet\"]:\n",
    "            distinct_entities.add(h)\n",
    "            distinct_entities.add(t)\n",
    "\n",
    "    return updated_data, list(distinct_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354bfe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: d:\\claimpkg\\claimpkg-clone\\src\\notebooks\\..\\resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 5000/5000 [00:00<00:00, 95791.42it/s]\n",
      "Processing data: 100%|██████████| 1000/1000 [00:00<00:00, 93111.57it/s]\n",
      "Processing data: 100%|██████████| 300/300 [00:00<00:00, 54337.40it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'resources'\n",
    "# Data dir = (1) working directory, (2) move out of test, (3) move out of src, and append to resources\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'resources')\n",
    "print(\"Data Directory:\", DATA_DIR)\n",
    "\n",
    "TRAIN_FILE = 'factkg_train_5k.pickle'\n",
    "TEST_FILE = 'factkg_test_1k.pickle'\n",
    "VALID_FILE = 'factkg_val_300.pickle'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "VALID_FILE_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
    "\n",
    "import pickle\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "valid_data = None\n",
    "with open(TRAIN_FILE_PATH, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(TEST_FILE_PATH, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open(VALID_FILE_PATH, 'rb') as f:\n",
    "    valid_data = pickle.load(f)\n",
    "\n",
    "train_updated_data, train_distinct_entities = process_data(train_data, remove_underscore=True)\n",
    "\n",
    "test_updated_data, test_distinct_entities = process_data(test_data, remove_underscore=True)\n",
    "\n",
    "valid_updated_data, valid_distinct_entities = process_data(valid_data, remove_underscore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b76c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items in concatenated data: 6300\n"
     ]
    }
   ],
   "source": [
    "# Concat the 3 set train, test, valid\n",
    "concat_data = {}\n",
    "concat_data.update(train_updated_data)\n",
    "concat_data.update(test_updated_data)\n",
    "concat_data.update(valid_updated_data)\n",
    "\n",
    "print(\"Total number of items in concatenated data:\", len(concat_data))\n",
    "\n",
    "key_list = list(concat_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0781a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct types found: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['coll:presup',\n",
       " 'coll:model',\n",
       " 'question',\n",
       " 'existence',\n",
       " 'num4',\n",
       " 'written',\n",
       " 'num3',\n",
       " 'num2',\n",
       " 'multi hop',\n",
       " 'negation',\n",
       " 'substitution',\n",
       " 'num1',\n",
       " 'multi claim']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all types items from train, test, and valid datasets\n",
    "types = set()\n",
    "for data in [train_updated_data, test_updated_data, valid_updated_data]:\n",
    "    for item in data:\n",
    "        for element in data[item]['types']:\n",
    "            types.add(element)\n",
    "types = list(types)\n",
    "print(\"Distinct types found:\", len(types))\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b7535",
   "metadata": {},
   "source": [
    "# Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31592f3",
   "metadata": {},
   "source": [
    "| Type           | Category            | Ý nghĩa                                |\n",
    "| -------------- | ------------------- | -------------------------------------- |\n",
    "| `written`      | claim style         | Văn phong tự nhiên                     |\n",
    "| `coll:model`   | claim style         | Văn nói do model sinh                  |\n",
    "| `coll:presup`  | claim style         | Dạng câu hỏi giả định (presupposition) |\n",
    "| `num1`         | reasoning           | One-hop                                |\n",
    "| `multi claim`  | reasoning           | Contains multiple facts                |\n",
    "| `existence`    | reasoning           | Hỏi về sự tồn tại                      |\n",
    "| `multi hop`    | reasoning           | Multi-hop reasoning                    |\n",
    "| `negation`     | reasoning           | Phủ định                               |\n",
    "| `num2`         | reasoning (complex) | Multi relation 2 chiều                 |\n",
    "| `num3`         | reasoning (complex) | Multi relation 3 chiều                 |\n",
    "| `substitution` | generation          | Claim tạo bằng thay thế thông tin      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec6a4a",
   "metadata": {},
   "source": [
    "So, these won't work or need to concern.\n",
    "(1) existence\n",
    "(2) num[```i```]\n",
    "(3) multi-hop\n",
    "\n",
    "The next step is to find number of rows to concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6730aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of concerned types: 4066\n",
      "Total types checked: 6300\n"
     ]
    }
   ],
   "source": [
    "is_concern = []\n",
    "# Get all types items from train, test, and valid datasets\n",
    "for data in [train_updated_data, test_updated_data, valid_updated_data]:\n",
    "    for item in data:\n",
    "        if 'multi-hop' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        if 'num2' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        if 'num3' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        if 'num4' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        is_concern.append(False)\n",
    "\n",
    "print(\"Number of concerned types:\", sum(is_concern))\n",
    "print(\"Total types checked:\", len(is_concern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674d6735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concern state: False\n",
      "Key: It was Micol Fontana who did not have an award.\n",
      "DATA: {'Label': [True], 'Entity_set': ['Micol_Fontana'], 'Evidence': {'Micol_Fontana': [['award']]}, 'types': ['coll:model', 'negation', 'existence'], 'triplet': []}\n"
     ]
    }
   ],
   "source": [
    "INDEX = 20\n",
    "print(f\"Concern state: {is_concern[INDEX]}\")\n",
    "print(f\"Key: {key_list[INDEX]}\")\n",
    "print(f\"DATA: {concat_data[key_list[INDEX]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb9c45",
   "metadata": {},
   "source": [
    "# Analyzing the difficult level of multi-hop or num-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556b8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PRONOUNS = {\"he\", \"she\", \"they\", \"them\", \"his\", \"her\", \"their\",\n",
    "            \"its\", \"it\", \"this artist\", \"the artist\", \"the city\",\n",
    "            \"the governor\", \"the musician\"}\n",
    "\n",
    "def contains_pronoun(claim: str):\n",
    "    text = claim.lower()\n",
    "    return any(p in text for p in PRONOUNS)\n",
    "\n",
    "def classify_multihop_complexity(claim: str, sample: dict):\n",
    "    \"\"\"\n",
    "    Classify multi-hop difficulty into: easy, medium, hard\n",
    "    \"\"\"\n",
    "    types = sample[\"types\"]\n",
    "    evidence = sample.get(\"Evidence\")\n",
    "\n",
    "    # Not multi-hop => always easy\n",
    "    if \"multi hop\" not in types:\n",
    "        return \"easy\"\n",
    "\n",
    "    # Count relations per entity\n",
    "    rel_counts = {ent: len(paths) for ent, paths in evidence.items()}\n",
    "\n",
    "    # Count relation-types inside each hop\n",
    "    hop_complexity = []\n",
    "    for ent, paths in evidence.items():\n",
    "        for hop in paths:\n",
    "            hop_complexity.append(len(hop))\n",
    "\n",
    "    max_rels_per_entity = max(rel_counts.values()) if rel_counts else 0\n",
    "    max_hop_width = max(hop_complexity) if hop_complexity else 0\n",
    "\n",
    "    # Number of entities\n",
    "    num_entities = len(evidence)\n",
    "\n",
    "    # Detect cycles -- if entity A has ~r followed by B has r again\n",
    "    def has_inverse_cycles(evidence):\n",
    "        inverse_pairs = {}\n",
    "        for ent, paths in evidence.items():\n",
    "            for hop in paths:\n",
    "                for rel in hop:\n",
    "                    if rel.startswith(\"~\"):\n",
    "                        inverse_pairs.setdefault(ent, []).append(rel[1:])\n",
    "        # If multiple entities share inverse forms => likely cycle\n",
    "        inverse_map = {}\n",
    "        for ent, relations in inverse_pairs.items():\n",
    "            for r in relations:\n",
    "                inverse_map.setdefault(r, []).append(ent)\n",
    "        return any(len(v) > 1 for v in inverse_map.values())\n",
    "\n",
    "    # Rule: Easy cases\n",
    "    if max_rels_per_entity == 1 and max_hop_width == 1:\n",
    "        # no ambiguity, no complex inverse chains\n",
    "        return \"easy\"\n",
    "\n",
    "    # Rule: Hard cases (requiring GPT)\n",
    "    # 1. Implicit subject/object → pronoun\n",
    "    if contains_pronoun(claim):\n",
    "        return \"hard\"\n",
    "\n",
    "    # 2. Entity has > 2 relation paths → ambiguous multi-hop (num2, num3)\n",
    "    if max_rels_per_entity >= 3:\n",
    "        return \"hard\"\n",
    "\n",
    "    # 3. Any hop uses >= 3 relations (multi-path)\n",
    "    if max_hop_width >= 3:\n",
    "        return \"hard\"\n",
    "\n",
    "    # 4. Inverse cycles or loops\n",
    "    if has_inverse_cycles(evidence):\n",
    "        return \"hard\"\n",
    "\n",
    "    # 5. Too many entities in multi-hop (structure ambiguous)\n",
    "    if num_entities >= 4:\n",
    "        return \"hard\"\n",
    "\n",
    "    # If not easy, not hard → medium\n",
    "    return \"medium\"\n",
    "\n",
    "complexity_counts = {\"easy\": [], \"medium\": [], \"hard\": []}\n",
    "\n",
    "for i, key in enumerate(concat_data):\n",
    "    if is_concern[i] == False:\n",
    "        continue\n",
    "\n",
    "    item = concat_data[key]\n",
    "    difficulty = classify_multihop_complexity(key, item)\n",
    "    complexity_counts[difficulty].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b766c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy: 2315 items (56.94%)\n",
      "Medium: 113 items (2.78%)\n",
      "Hard: 1638 items (40.29%)\n"
     ]
    }
   ],
   "source": [
    "# Count and show percentages\n",
    "total_counts = sum(len(v) for v in complexity_counts.values())\n",
    "for level, items in complexity_counts.items():\n",
    "    count = len(items)\n",
    "    percentage = (count / total_counts) * 100 if total_counts > 0 else 0\n",
    "    print(f\"{level.capitalize()}: {count} items ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8e50f",
   "metadata": {},
   "source": [
    "# Inspecting each classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b1442d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Born on April 27, 1937 and died on December 9th,1991.\n",
      "DATA: {'Label': [True], 'Entity_set': ['\"1991-12-09\"', '\"1937-04-27\"'], 'Evidence': {'\"1937-04-27\"': [['~birthDate', 'deathDate']], '\"1991-12-09\"': [['~deathDate', 'birthDate']]}, 'types': ['coll:model', 'num2', 'multi hop'], 'triplet': [('unknown_0', 'birthDate', '\"1937-04-27\"'), ('unknown_0', 'deathDate', '\"1991-12-09\"'), ('unknown_1', 'deathDate', '\"1991-12-09\"'), ('unknown_1', 'birthDate', '\"1937-04-27\"'), ('\"1991-12-09\"', 'birthDate', '\"1937-04-27\"'), ('\"1937-04-27\"', 'deathDate', '\"1991-12-09\"')]}\n"
     ]
    }
   ],
   "source": [
    "INDEX = 3\n",
    "key = complexity_counts['medium'][INDEX]\n",
    "print(f\"Key: {key}\")\n",
    "print(f\"DATA: {concat_data[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feafbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Fix - Key: Ahmet Ertegun is from a country that has an ethnic group called African Americans.\n",
      "DATA: {'Label': [True], 'Entity_set': ['Ahmet_Ertegun', 'African_Americans'], 'Evidence': {'Ahmet_Ertegun': [['hometown', 'ethnicGroup']], 'African_Americans': [['~ethnicGroup', '~hometown']]}, 'types': ['coll:model', 'num2', 'multi hop'], 'triplet': [('Ahmet Ertegun', 'hometown', 'unknown_0'), ('unknown_0', 'ethnicGroup', 'African Americans'), ('unknown_1', 'ethnicGroup', 'African Americans'), ('African Americans', 'hometown', 'unknown_1'), ('Ahmet Ertegun', 'hometown', 'African Americans'), ('Ahmet Ertegun', 'ethnicGroup', 'African Americans')]}\n",
      "After Fix - Triplets: [('Ahmet Ertegun', 'hometown', 'unknown_0'), ('unknown_0', 'ethnicGroup', 'African Americans')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "_UNKNOWN_RE = re.compile(r\"^unknown[\\s_\\-]?(\\d+)$\", flags=re.I)\n",
    "\n",
    "def _is_unknown_token(s: str):\n",
    "    m = _UNKNOWN_RE.match(str(s))\n",
    "    return bool(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _unknown_index(s: str):\n",
    "    m = _UNKNOWN_RE.match(str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _find_representative_unknown(triplets: List[Tuple[str,str,str]], keep_idx: int):\n",
    "    \"\"\"\n",
    "    Find an existing string in triplets that corresponds to unknown with index keep_idx,\n",
    "    preserving original formatting (underscore/space/hyphen).\n",
    "    If not found, return canonical 'unknown_0' style.\n",
    "    \"\"\"\n",
    "    for x,y,z in triplets:\n",
    "        for token in (x,y,z):\n",
    "            if _is_unknown_token(token) and _unknown_index(token) == keep_idx:\n",
    "                return token\n",
    "    return f\"unknown_{keep_idx}\"\n",
    "\n",
    "def apply_fix_medium(sample: Dict[str, Any],\n",
    "                     is_eliminate_reverse: bool = True,\n",
    "                     remove_underscore: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fix medium-case triplets in-place (updates sample[\"triplet\"]).\n",
    "    - Keeps only unknown_0 related triplets (and canonicalizes unknown token)\n",
    "    - Removes unknown_i (i>0)\n",
    "    - Removes direct entity->entity edges when unknown_0 exists\n",
    "    - If is_eliminate_reverse=True, removes entity->rel->unknown_0 triples (reverse)\n",
    "    \"\"\"\n",
    "\n",
    "    triplets = list(sample.get(\"triplet\", []))\n",
    "    evidence = sample.get(\"Evidence\", {})\n",
    "\n",
    "    # Normalize function for comparison if needed\n",
    "    def norm_name(s: str) -> str:\n",
    "        if not isinstance(s, str):\n",
    "            return s\n",
    "        if remove_underscore:\n",
    "            return s.replace(\"_\", \" \")\n",
    "        return s\n",
    "\n",
    "    # 1) detect any unknown indices present\n",
    "    used_idxs = set()\n",
    "    for h,r,t in triplets:\n",
    "        if _is_unknown_token(h):\n",
    "            idx = _unknown_index(h)\n",
    "            if idx is not None:\n",
    "                used_idxs.add(idx)\n",
    "        if _is_unknown_token(t):\n",
    "            idx = _unknown_index(t)\n",
    "            if idx is not None:\n",
    "                used_idxs.add(idx)\n",
    "\n",
    "    if not used_idxs:\n",
    "        # Nothing to do: no unknown tokens present. Just return sample unchanged.\n",
    "        return sample\n",
    "\n",
    "    # choose keep index = smallest used index (prefer unknown_0 if present)\n",
    "    keep_idx = min(used_idxs)\n",
    "\n",
    "    # find representative string for keep unknown (to preserve formatting)\n",
    "    rep_unknown = _find_representative_unknown(triplets, keep_idx)\n",
    "\n",
    "    # Build set of entities that appear ONLY as ~relations in evidence (tail-only)\n",
    "    ev_map = { norm_name(k): v for k,v in evidence.items() }\n",
    "    tail_only_entities = set()\n",
    "    for ent, groups in ev_map.items():\n",
    "        has_fwd = False\n",
    "        has_inv = False\n",
    "        for g in groups:\n",
    "            for rel in g:\n",
    "                if isinstance(rel, str) and rel.startswith(\"~\"):\n",
    "                    has_inv = True\n",
    "                else:\n",
    "                    has_fwd = True\n",
    "        if has_inv and not has_fwd:\n",
    "            tail_only_entities.add(ent)\n",
    "\n",
    "    # 2) Filter triplets:\n",
    "    cleaned: List[Tuple[str,str,str]] = []\n",
    "    for h, r, t in triplets:\n",
    "        h_s = str(h)\n",
    "        t_s = str(t)\n",
    "\n",
    "        # A) Remove any unknown token whose index != keep_idx\n",
    "        if _is_unknown_token(h_s):\n",
    "            if _unknown_index(h_s) != keep_idx:\n",
    "                continue\n",
    "        if _is_unknown_token(t_s):\n",
    "            if _unknown_index(t_s) != keep_idx:\n",
    "                continue\n",
    "\n",
    "        # B) If both head and tail are non-unknown (direct entity->entity), drop it\n",
    "        if (not _is_unknown_token(h_s)) and (not _is_unknown_token(t_s)):\n",
    "            # If unknown_0 exists anywhere, we drop direct edges to avoid bypass.\n",
    "            # We enforce dropping direct edges here (Option A).\n",
    "            continue\n",
    "\n",
    "        # C) If eliminate_reverse and triplet is entity -> rel -> unknown_0, drop it\n",
    "        #     (i.e., head is non-unknown, tail is the keep unknown rep)\n",
    "        if is_eliminate_reverse:\n",
    "            if (not _is_unknown_token(h_s)) and (_is_unknown_token(t_s) and _unknown_index(t_s)==keep_idx):\n",
    "                continue\n",
    "\n",
    "        # D) Normalize the keep unknown token to the representative string\n",
    "        #    so final output uses consistent unknown token (rep_unknown).\n",
    "        h_out = h_s\n",
    "        t_out = t_s\n",
    "        if _is_unknown_token(h_out) and _unknown_index(h_out) == keep_idx:\n",
    "            h_out = rep_unknown\n",
    "        if _is_unknown_token(t_out) and _unknown_index(t_out) == keep_idx:\n",
    "            t_out = rep_unknown\n",
    "\n",
    "        cleaned.append((h_out, r, t_out))\n",
    "\n",
    "    # 3) Deduplicate preserving order\n",
    "    final = []\n",
    "    seen = set()\n",
    "    for tpl in cleaned:\n",
    "        if tpl not in seen:\n",
    "            seen.add(tpl)\n",
    "            final.append(tpl)\n",
    "\n",
    "    # 4) If final is empty but evidence suggests we should keep a chain,\n",
    "    #    attempt to construct minimal unknown_0 chain from evidence:\n",
    "    #    (This is a safe fallback, but typically not needed.)\n",
    "    if not final:\n",
    "        # Build simple chain: find an entity that has forward rels and match partner with ~rel\n",
    "        # We'll create rep_unknown -> rel -> partner for such cases\n",
    "        # This fallback is conservative.\n",
    "        for ent, groups in ev_map.items():\n",
    "            for rel_group in groups:\n",
    "                # consider only multi-hop/one-hop forward relations\n",
    "                for rel in rel_group:\n",
    "                    inv = isinstance(rel,str) and rel.startswith(\"~\")\n",
    "                    rel_clean = rel[1:] if inv else rel\n",
    "                    if not inv:\n",
    "                        # find partner that has ~rel\n",
    "                        partner = None\n",
    "                        for e2, g2 in ev_map.items():\n",
    "                            if e2 == ent: continue\n",
    "                            for g2part in g2:\n",
    "                                if f\"~{rel_clean}\" in g2part:\n",
    "                                    partner = e2\n",
    "                                    break\n",
    "                            if partner: break\n",
    "                        if partner:\n",
    "                            final.append((ent, rel_clean, rep_unknown))\n",
    "                            final.append((rep_unknown, \"ethnicGroup\" if \"ethnic\" in rel_clean else rel_clean, partner))\n",
    "                            break\n",
    "            if final:\n",
    "                break\n",
    "\n",
    "    sample[\"triplet\"] = final\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "# def apply_fix_medium(sample, is_eliminate_reverse=True, remove_underscore=True):\n",
    "#     \"\"\"\n",
    "#     Medium-level fixing of triplets generated by generate_basic.\n",
    "#     - Supports 'unknown_0' AND 'unknown 0'\n",
    "#     - Removes reverse-direction triples\n",
    "#     - Removes spurious unknown_1+\n",
    "#     - Keeps only the correct multi-hop structure\n",
    "#     \"\"\"\n",
    "\n",
    "#     triplets = sample[\"triplet\"]\n",
    "#     evidence = sample[\"Evidence\"]\n",
    "\n",
    "#     # Normalize entities\n",
    "#     def norm(x):\n",
    "#         if isinstance(x, str) and remove_underscore:\n",
    "#             return x.replace(\"_\", \" \")\n",
    "#         return x\n",
    "\n",
    "#     # Identify all unknowns: matches \"unknown_0\", \"unknown 0\", \"unknown-0\"\n",
    "#     UNKNOWN_RE = re.compile(r\"^unknown[\\s_]?(\\d+)$\")\n",
    "\n",
    "#     def is_unknown(x):\n",
    "#         return isinstance(x, str) and UNKNOWN_RE.match(x) is not None\n",
    "\n",
    "#     def unknown_index(x):\n",
    "#         m = UNKNOWN_RE.match(x)\n",
    "#         return int(m.group(1)) if m else None\n",
    "\n",
    "#     # Normalize evidence keys\n",
    "#     ev = {norm(k): v for k, v in evidence.items()}\n",
    "\n",
    "#     # ---------------------------------------------------\n",
    "#     # STEP 1 — Find tail-only entities (only have ~rel)\n",
    "#     # ---------------------------------------------------\n",
    "#     tail_only_entities = set()\n",
    "#     for ent, groups in ev.items():\n",
    "#         has_forward = False\n",
    "#         has_inverse = False\n",
    "#         for g in groups:\n",
    "#             for r in g:\n",
    "#                 if r.startswith(\"~\"):\n",
    "#                     has_inverse = True\n",
    "#                 else:\n",
    "#                     has_forward = True\n",
    "#         if is_eliminate_reverse and has_inverse and not has_forward:\n",
    "#             tail_only_entities.add(norm(ent))\n",
    "\n",
    "#     # ---------------------------------------------------\n",
    "#     # STEP 2 — Identify all unknown indexes used\n",
    "#     # ---------------------------------------------------\n",
    "#     used_unknowns = set()\n",
    "#     for h, r, t in triplets:\n",
    "#         if is_unknown(h):\n",
    "#             used_unknowns.add(unknown_index(h))\n",
    "#         if is_unknown(t):\n",
    "#             used_unknowns.add(unknown_index(t))\n",
    "\n",
    "#     # Keep only the *lowest* unknown index (usually unknown_0)\n",
    "#     keep_idx = min(used_unknowns) if used_unknowns else None\n",
    "\n",
    "#     # ---------------------------------------------------\n",
    "#     # STEP 3 — Filter triplets\n",
    "#     # ---------------------------------------------------\n",
    "#     cleaned = []\n",
    "\n",
    "#     for h, r, t in triplets:\n",
    "\n",
    "#         h_n = norm(h) if isinstance(h, str) else h\n",
    "#         t_n = norm(t) if isinstance(t, str) else t\n",
    "\n",
    "#         # A) Remove unknown_i where i > keep_idx\n",
    "#         if is_unknown(h_n):\n",
    "#             if unknown_index(h_n) != keep_idx:\n",
    "#                 continue\n",
    "#         if is_unknown(t_n):\n",
    "#             if unknown_index(t_n) != keep_idx:\n",
    "#                 continue\n",
    "\n",
    "#         # B) Remove reverse-direction triples\n",
    "#         if is_eliminate_reverse and h_n in tail_only_entities:\n",
    "#             continue\n",
    "\n",
    "#         # C) Remove direct Ahmet → African Americans that bypass unknown\n",
    "#         if keep_idx is not None and t_n in tail_only_entities and not is_unknown(t_n):\n",
    "#             # remove h → tail-only entity direct if unknown exists\n",
    "#             if not is_unknown(h_n):\n",
    "#                 continue\n",
    "\n",
    "#         cleaned.append((h_n, r, t_n))\n",
    "\n",
    "#     # ---------------------------------------------------\n",
    "#     # STEP 4 — Deduplicate preserving order\n",
    "#     # ---------------------------------------------------\n",
    "#     final = []\n",
    "#     seen = set()\n",
    "#     for tpl in cleaned:\n",
    "#         if tpl not in seen:\n",
    "#             seen.add(tpl)\n",
    "#             final.append(tpl)\n",
    "\n",
    "#     sample[\"triplet\"] = final\n",
    "#     return sample\n",
    "\n",
    "\n",
    "# Test the fix on a medium complexity example\n",
    "INDEX = 2\n",
    "key = complexity_counts['medium'][INDEX]\n",
    "print(f\"Before Fix - Key: {key}\")\n",
    "print(f\"DATA: {concat_data[key]}\")\n",
    "\n",
    "after_fix_sample = apply_fix_medium(concat_data[key], is_eliminate_reverse=False)\n",
    "after_fix = after_fix_sample['triplet']\n",
    "print(f\"After Fix - Triplets: {after_fix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
