{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad37a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6c67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "remove_underscore = True\n",
    "\n",
    "def _norm(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    return s.replace(\"_\", \" \") if remove_underscore else s\n",
    "\n",
    "def _new_unknown(unk_list):\n",
    "    u = f\"unknown_{len(unk_list)}\"\n",
    "    unk_list.append(u)\n",
    "    return u\n",
    "\n",
    "def generate(claim: str, sample: Dict[str, Any]):\n",
    "    # Normalize entities by adding the underscore\n",
    "    entity_set = [_norm(e) for e in sample.get(\"Entity_set\", [])]\n",
    "\n",
    "    # Get evidences\n",
    "    evidence = sample.get(\"Evidence\", {})\n",
    "\n",
    "    # Normalize keys\n",
    "    evidence_map = { _norm(k): v for k, v in evidence.items() }\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 1 — Build direct matches (A --rel--> B)\n",
    "    # --------------------------------------------------------------\n",
    "    direct_edges = []\n",
    "\n",
    "    for e1, groups1 in evidence_map.items():\n",
    "        for rels1 in groups1:\n",
    "            for r in rels1:\n",
    "                inverse = r.startswith(\"~\")\n",
    "                rel = r[1:] if inverse else r\n",
    "\n",
    "                # CASE 1: forward r → look for entity with ~r\n",
    "                if not inverse:\n",
    "                    for e2, groups2 in evidence_map.items():\n",
    "                        if e1 == e2: continue\n",
    "                        for rels2 in groups2:\n",
    "                            if f\"~{rel}\" in rels2:\n",
    "                                direct_edges.append((e1, rel, e2))\n",
    "\n",
    "                # CASE 2: inverse ~r → look for entity with forward r\n",
    "                else:\n",
    "                    for e2, groups2 in evidence_map.items():\n",
    "                        if e1 == e2: continue\n",
    "                        for rels2 in groups2:\n",
    "                            if rel in rels2:\n",
    "                                direct_edges.append((e2, rel, e1))\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 2 — Now build multi-hop paths (using unknown_i)\n",
    "    # --------------------------------------------------------------\n",
    "    unknown_list = []\n",
    "    triplets = []\n",
    "\n",
    "    # dictionary to quickly check for direct pairs\n",
    "    direct_set = set((h, r, t) for h, r, t in direct_edges)\n",
    "\n",
    "    # Helper to detect if last hop of unit should link directly\n",
    "    def find_direct_target(rel, exclude):\n",
    "        for h, r, t in direct_edges:\n",
    "            if r == rel:\n",
    "                if h != exclude:\n",
    "                    return t\n",
    "        return None\n",
    "\n",
    "    for ent, rel_groups in evidence_map.items():\n",
    "        for rel_path in rel_groups:\n",
    "\n",
    "            # If path has only 1 rel, and direct match exists → already captured\n",
    "            if len(rel_path) == 1:\n",
    "                r = rel_path[0]\n",
    "                inv = r.startswith(\"~\")\n",
    "                rel = r[1:] if inv else r\n",
    "\n",
    "                # direct already handled → skip\n",
    "                continue\n",
    "\n",
    "            # Multi-hop path → must use unknowns\n",
    "            curr = ent\n",
    "            for i, r in enumerate(rel_path):\n",
    "                inv = r.startswith(\"~\")\n",
    "                rel = r[1:] if inv else r\n",
    "\n",
    "                # last hop? try direct match\n",
    "                if i == len(rel_path) - 1:\n",
    "                    target = find_direct_target(rel, curr)\n",
    "                    if target is None:\n",
    "                        target = _new_unknown(unknown_list)\n",
    "                else:\n",
    "                    # internal hop → always unknown\n",
    "                    target = _new_unknown(unknown_list)\n",
    "\n",
    "                # build triplet\n",
    "                if inv:\n",
    "                    triplets.append((target, rel, curr))\n",
    "                else:\n",
    "                    triplets.append((curr, rel, target))\n",
    "\n",
    "                curr = target\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 3 — Combine direct + multi-hop, deduplicate\n",
    "    # --------------------------------------------------------------\n",
    "    triplets.extend(direct_edges)\n",
    "    final = list(dict.fromkeys(triplets))\n",
    "\n",
    "    sample[\"triplet\"] = final\n",
    "    return sample\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "def linearize(triplets: List[Tuple[str, str, str]]) -> str:\n",
    "    return \"\\n\".join(f\"<e>{h}</e> || {r} || <e>{t}</e>\" for h, r, t in triplets)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(data: dict, remove_underscore: bool = True) -> Tuple[Dict, List]:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    updated_data = {}\n",
    "    distinct_entities = set()\n",
    "    keys = list(data.keys())\n",
    "\n",
    "    for key in tqdm(keys, desc=\"Processing data\"):\n",
    "        updated = generate(key, data[key])\n",
    "        updated_data[key] = updated\n",
    "\n",
    "        for h, r, t in updated[\"triplet\"]:\n",
    "            distinct_entities.add(h)\n",
    "            distinct_entities.add(t)\n",
    "\n",
    "    return updated_data, list(distinct_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354bfe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: d:\\claimpkg\\claimpkg-clone\\src\\notebooks\\..\\resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 5000/5000 [00:00<00:00, 30026.62it/s]\n",
      "Processing data: 100%|██████████| 1000/1000 [00:00<00:00, 86544.73it/s]\n",
      "Processing data: 100%|██████████| 300/300 [00:00<00:00, 73292.82it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'resources'\n",
    "# Data dir = (1) working directory, (2) move out of test, (3) move out of src, and append to resources\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'resources')\n",
    "print(\"Data Directory:\", DATA_DIR)\n",
    "\n",
    "TRAIN_FILE = 'factkg_train_5k.pickle'\n",
    "TEST_FILE = 'factkg_test_1k.pickle'\n",
    "VALID_FILE = 'factkg_val_300.pickle'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "VALID_FILE_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
    "\n",
    "import pickle\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "valid_data = None\n",
    "with open(TRAIN_FILE_PATH, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(TEST_FILE_PATH, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open(VALID_FILE_PATH, 'rb') as f:\n",
    "    valid_data = pickle.load(f)\n",
    "\n",
    "train_updated_data, train_distinct_entities = process_data(train_data, remove_underscore=True)\n",
    "\n",
    "test_updated_data, test_distinct_entities = process_data(test_data, remove_underscore=True)\n",
    "\n",
    "valid_updated_data, valid_distinct_entities = process_data(valid_data, remove_underscore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b76c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items in concatenated data: 6300\n"
     ]
    }
   ],
   "source": [
    "# Concat the 3 set train, test, valid\n",
    "concat_data = {}\n",
    "concat_data.update(train_updated_data)\n",
    "concat_data.update(test_updated_data)\n",
    "concat_data.update(valid_updated_data)\n",
    "\n",
    "print(\"Total number of items in concatenated data:\", len(concat_data))\n",
    "\n",
    "key_list = list(concat_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0781a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct types found: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['num2',\n",
       " 'num4',\n",
       " 'multi claim',\n",
       " 'negation',\n",
       " 'num3',\n",
       " 'coll:model',\n",
       " 'num1',\n",
       " 'existence',\n",
       " 'written',\n",
       " 'multi hop',\n",
       " 'coll:presup',\n",
       " 'substitution',\n",
       " 'question']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all types items from train, test, and valid datasets\n",
    "types = set()\n",
    "for data in [train_updated_data, test_updated_data, valid_updated_data]:\n",
    "    for item in data:\n",
    "        for element in data[item]['types']:\n",
    "            types.add(element)\n",
    "types = list(types)\n",
    "print(\"Distinct types found:\", len(types))\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b7535",
   "metadata": {},
   "source": [
    "# Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31592f3",
   "metadata": {},
   "source": [
    "| Type           | Category            | Ý nghĩa                                |\n",
    "| -------------- | ------------------- | -------------------------------------- |\n",
    "| `written`      | claim style         | Văn phong tự nhiên                     |\n",
    "| `coll:model`   | claim style         | Văn nói do model sinh                  |\n",
    "| `coll:presup`  | claim style         | Dạng câu hỏi giả định (presupposition) |\n",
    "| `num1`         | reasoning           | One-hop                                |\n",
    "| `multi claim`  | reasoning           | Contains multiple facts                |\n",
    "| `existence`    | reasoning           | Hỏi về sự tồn tại                      |\n",
    "| `multi hop`    | reasoning           | Multi-hop reasoning                    |\n",
    "| `negation`     | reasoning           | Phủ định                               |\n",
    "| `num2`         | reasoning (complex) | Multi relation 2 chiều                 |\n",
    "| `num3`         | reasoning (complex) | Multi relation 3 chiều                 |\n",
    "| `substitution` | generation          | Claim tạo bằng thay thế thông tin      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec6a4a",
   "metadata": {},
   "source": [
    "So, these won't work or need to concern.\n",
    "(1) existence\n",
    "(2) num[```i```]\n",
    "(3) multi-hop\n",
    "\n",
    "The next step is to find number of rows to concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6730aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of concerned types: 4066\n",
      "Total types checked: 6300\n",
      "Example Key: is published by Lippincott Williams & Wilkins in the UK where English is the main language.\n",
      "DATA: {'Label': [True], 'Entity_set': ['United_Kingdom', 'English_language', 'Lippincott_Williams_&_Wilkins', 'AIDS_(journal)'], 'Evidence': {'United_Kingdom': [['language'], ['~country']], 'AIDS_(journal)': [['country'], ['publisher']], 'English_language': [['~language']], 'Lippincott_Williams_&_Wilkins': [['~publisher']]}, 'types': ['coll:model', 'num4', 'multi claim'], 'triplet': [('United Kingdom', 'language', 'English language'), ('AIDS (journal)', 'country', 'United Kingdom'), ('AIDS (journal)', 'publisher', 'Lippincott Williams & Wilkins')], 'is_concern': True, 'complexity': 'skipped'}\n"
     ]
    }
   ],
   "source": [
    "is_concern : list[bool] = []\n",
    "# Get all types items from train, test, and valid datasets\n",
    "for data in [train_updated_data, test_updated_data, valid_updated_data]:\n",
    "    for item in data:\n",
    "        if 'multi-hop' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        if 'num2' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        if 'num3' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        if 'num4' in data[item]['types']:\n",
    "            is_concern.append(True)\n",
    "            continue\n",
    "        is_concern.append(False)\n",
    "\n",
    "print(\"Number of concerned types:\", sum(is_concern))\n",
    "print(\"Total types checked:\", len(is_concern))\n",
    "\n",
    "# Append the is_concern flag to the concat_data\n",
    "for idx, key in enumerate(key_list):\n",
    "    concat_data[key]['is_concern'] = is_concern[idx]\n",
    "    concat_data[key]['complexity'] = 'skipped'\n",
    "\n",
    "# See the example\n",
    "example_key = key_list[0]\n",
    "print(f\"Example Key: {example_key}\")\n",
    "print(f\"DATA: {concat_data[example_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674d6735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concern state: False\n",
      "Key: It was Micol Fontana who did not have an award.\n",
      "DATA: {'Label': [True], 'Entity_set': ['Micol_Fontana'], 'Evidence': {'Micol_Fontana': [['award']]}, 'types': ['coll:model', 'negation', 'existence'], 'triplet': [], 'is_concern': False, 'complexity': 'skipped'}\n"
     ]
    }
   ],
   "source": [
    "INDEX = 20\n",
    "print(f\"Concern state: {is_concern[INDEX]}\")\n",
    "print(f\"Key: {key_list[INDEX]}\")\n",
    "print(f\"DATA: {concat_data[key_list[INDEX]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb9c45",
   "metadata": {},
   "source": [
    "# Analyzing the difficult level of multi-hop or num-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556b8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PRONOUNS = {\"he\", \"she\", \"they\", \"them\", \"his\", \"her\", \"their\",\n",
    "            \"its\", \"it\", \"this artist\", \"the artist\", \"the city\",\n",
    "            \"the governor\", \"the musician\"}\n",
    "\n",
    "def contains_pronoun(claim: str):\n",
    "    text = claim.lower()\n",
    "    return any(p in text for p in PRONOUNS)\n",
    "\n",
    "def classify_multihop_complexity(claim: str, sample: dict):\n",
    "    \"\"\"\n",
    "    Classify multi-hop difficulty into: easy, medium, hard\n",
    "    \"\"\"\n",
    "    types = sample[\"types\"]\n",
    "    evidence = sample.get(\"Evidence\")\n",
    "\n",
    "    # Not multi-hop => always easy\n",
    "    if \"multi hop\" not in types:\n",
    "        return \"easy\"\n",
    "\n",
    "    # Count relations per entity\n",
    "    rel_counts = {ent: len(paths) for ent, paths in evidence.items()}\n",
    "\n",
    "    # Count relation-types inside each hop\n",
    "    hop_complexity = []\n",
    "    for ent, paths in evidence.items():\n",
    "        for hop in paths:\n",
    "            hop_complexity.append(len(hop))\n",
    "\n",
    "    max_rels_per_entity = max(rel_counts.values()) if rel_counts else 0\n",
    "    max_hop_width = max(hop_complexity) if hop_complexity else 0\n",
    "\n",
    "    # Number of entities\n",
    "    num_entities = len(evidence)\n",
    "\n",
    "    # Detect cycles -- if entity A has ~r followed by B has r again\n",
    "    def has_inverse_cycles(evidence):\n",
    "        inverse_pairs = {}\n",
    "        for ent, paths in evidence.items():\n",
    "            for hop in paths:\n",
    "                for rel in hop:\n",
    "                    if rel.startswith(\"~\"):\n",
    "                        inverse_pairs.setdefault(ent, []).append(rel[1:])\n",
    "        # If multiple entities share inverse forms => likely cycle\n",
    "        inverse_map = {}\n",
    "        for ent, relations in inverse_pairs.items():\n",
    "            for r in relations:\n",
    "                inverse_map.setdefault(r, []).append(ent)\n",
    "        return any(len(v) > 1 for v in inverse_map.values())\n",
    "\n",
    "    # Rule: Easy cases\n",
    "    if max_rels_per_entity == 1 and max_hop_width == 1:\n",
    "        # no ambiguity, no complex inverse chains\n",
    "        return \"easy\"\n",
    "\n",
    "    # Rule: Hard cases (requiring GPT)\n",
    "    # 1. Implicit subject/object → pronoun\n",
    "    if contains_pronoun(claim):\n",
    "        return \"hard\"\n",
    "\n",
    "    # 2. Entity has > 2 relation paths → ambiguous multi-hop (num2, num3)\n",
    "    if max_rels_per_entity >= 3:\n",
    "        return \"hard\"\n",
    "\n",
    "    # 3. Any hop uses >= 3 relations (multi-path)\n",
    "    if max_hop_width >= 3:\n",
    "        return \"hard\"\n",
    "\n",
    "    # 4. Inverse cycles or loops\n",
    "    if has_inverse_cycles(evidence):\n",
    "        return \"hard\"\n",
    "\n",
    "    # 5. Too many entities in multi-hop (structure ambiguous)\n",
    "    if num_entities >= 4:\n",
    "        return \"hard\"\n",
    "\n",
    "    # If not easy, not hard → medium\n",
    "    return \"medium\"\n",
    "\n",
    "complexity_counts = {\"easy\": [], \"medium\": [], \"hard\": []}\n",
    "\n",
    "for i, key in enumerate(concat_data):\n",
    "    if is_concern[i] == False:\n",
    "        continue\n",
    "\n",
    "    item = concat_data[key]\n",
    "    difficulty = classify_multihop_complexity(key, item)\n",
    "    complexity_counts[difficulty].append(key)\n",
    "    concat_data[key]['complexity'] = difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b766c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy: 2315 items (56.94%)\n",
      "Medium: 113 items (2.78%)\n",
      "Hard: 1638 items (40.29%)\n"
     ]
    }
   ],
   "source": [
    "# Count and show percentages\n",
    "total_counts = sum(len(v) for v in complexity_counts.values())\n",
    "for level, items in complexity_counts.items():\n",
    "    count = len(items)\n",
    "    percentage = (count / total_counts) * 100 if total_counts > 0 else 0\n",
    "    print(f\"{level.capitalize()}: {count} items ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8e50f",
   "metadata": {},
   "source": [
    "# Inspecting each classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1442d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Paleobiology is the academic discipline of an academic journal (abbreviated to Acta Palaeontol. Pol) which has the ISSN number 0567-7920.\n",
      "DATA: {'Label': [True], 'Entity_set': ['\"0567-7920\"', 'Paleobiology', '\"Acta Palaeontol. Pol.\"'], 'Evidence': {'\"Acta Palaeontol. Pol.\"': [['~abbreviation', 'discipline'], ['~abbreviation', 'issn']], 'Paleobiology': [['~discipline', 'abbreviation'], ['~discipline', 'issn']], '\"0567-7920\"': [['~issn', 'abbreviation'], ['~issn', 'discipline']]}, 'types': ['written', 'num3', 'multi hop'], 'triplet': [('unknown_0', 'abbreviation', '\"Acta Palaeontol. Pol.\"'), ('unknown_0', 'discipline', 'Paleobiology'), ('unknown_1', 'abbreviation', '\"Acta Palaeontol. Pol.\"'), ('unknown_1', 'issn', '\"0567-7920\"'), ('unknown_2', 'discipline', 'Paleobiology'), ('unknown_2', 'abbreviation', '\"Acta Palaeontol. Pol.\"'), ('unknown_3', 'discipline', 'Paleobiology'), ('unknown_3', 'issn', '\"0567-7920\"'), ('unknown_4', 'issn', '\"0567-7920\"'), ('unknown_4', 'abbreviation', '\"Acta Palaeontol. Pol.\"'), ('unknown_5', 'issn', '\"0567-7920\"'), ('unknown_5', 'discipline', 'Paleobiology'), ('Paleobiology', 'abbreviation', '\"Acta Palaeontol. Pol.\"'), ('\"0567-7920\"', 'abbreviation', '\"Acta Palaeontol. Pol.\"'), ('\"Acta Palaeontol. Pol.\"', 'discipline', 'Paleobiology'), ('\"Acta Palaeontol. Pol.\"', 'issn', '\"0567-7920\"'), ('\"0567-7920\"', 'discipline', 'Paleobiology'), ('Paleobiology', 'issn', '\"0567-7920\"')], 'is_concern': True, 'complexity': 'hard'}\n"
     ]
    }
   ],
   "source": [
    "INDEX = 3\n",
    "key = complexity_counts['hard'][INDEX]\n",
    "print(f\"Key: {key}\")\n",
    "print(f\"DATA: {concat_data[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f25a26",
   "metadata": {},
   "source": [
    "-> Easy set is good, but medium and hard set sucks. Let's extract it into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c93b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified data saved to d:\\claimpkg\\claimpkg-clone\\src\\notebooks\\..\\resources\\classified_book_dataset_6300.pickle\n"
     ]
    }
   ],
   "source": [
    "# Export the classified data into a pickle file\n",
    "OUTPUT_FILE = 'classified_book_dataset_6300.pickle'\n",
    "OUTPUT_PATH = os.path.join(DATA_DIR, OUTPUT_FILE)\n",
    "with open(OUTPUT_PATH, 'wb') as f:\n",
    "    pickle.dump(concat_data, f)\n",
    "print(f\"Classified data saved to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
