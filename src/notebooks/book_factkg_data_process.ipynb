{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f220eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ac3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "remove_underscore = True\n",
    "\n",
    "def _norm(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    return s.replace(\"_\", \" \") if remove_underscore else s\n",
    "\n",
    "def _new_unknown(unk_list):\n",
    "    u = f\"unknown_{len(unk_list)}\"\n",
    "    unk_list.append(u)\n",
    "    return u\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#          →→  CORRECT GRAPH CONSTRUCTION FOR FACTKG  ←←\n",
    "# ============================================================\n",
    "\n",
    "def generate(claim: str, sample: Dict[str, Any]):\n",
    "    entity_set = [_norm(e) for e in sample.get(\"Entity_set\", [])]\n",
    "    evidence = sample.get(\"Evidence\", {})\n",
    "\n",
    "    # Normalize keys\n",
    "    evidence_map = { _norm(k): v for k, v in evidence.items() }\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 1 — Create ONE hidden entity for the entire claim\n",
    "    # --------------------------------------------------------------\n",
    "    # All evidence paths describe the same hidden entity (FactKG rule)\n",
    "    hidden = \"unknown_0\"\n",
    "    unknown_list = [hidden]  # keep index consistency\n",
    "\n",
    "    triplets = []\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 2 — Build real targets (the tail of relations)\n",
    "    # The rule in FactKG:\n",
    "    # - If rel = r  → hidden --r--> target(entity with ~r)\n",
    "    # - If rel = ~r → target(entity with r) --r--> hidden\n",
    "    # --------------------------------------------------------------\n",
    "    def find_target(rel, inverse):\n",
    "        \"\"\"Find the target entity mentioned in evidence.\"\"\"\n",
    "        for cand, groups in evidence_map.items():\n",
    "            for g in groups:\n",
    "                if inverse:\n",
    "                    # rel = r, inverse = True → look for r in groups\n",
    "                    if rel in g:\n",
    "                        return cand\n",
    "                else:\n",
    "                    # normal forward rel → look for ~rel\n",
    "                    if f\"~{rel}\" in g:\n",
    "                        return cand\n",
    "        return None  # no explicit target → unspecified → hidden hop continuation\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # STEP 3 — Iterate over all evidence paths\n",
    "    # --------------------------------------------------------------\n",
    "    for ent, rel_groups in evidence_map.items():\n",
    "\n",
    "        for rel_path in rel_groups:\n",
    "\n",
    "            # pointer starts at hidden entity\n",
    "            curr = hidden\n",
    "\n",
    "            # traverse each relation in the path\n",
    "            for i, r in enumerate(rel_path):\n",
    "                inverse = r.startswith(\"~\")\n",
    "                rel = r[1:] if inverse else r\n",
    "\n",
    "                # Last hop should connect to REAL entity if exists\n",
    "                if i == len(rel_path) - 1:\n",
    "                    target = find_target(rel, inverse)\n",
    "                else:\n",
    "                    # internal hop → real entity not known → create one more hidden\n",
    "                    next_hidden = f\"unknown_{len(unknown_list)}\"\n",
    "                    unknown_list.append(next_hidden)\n",
    "                    target = next_hidden\n",
    "\n",
    "                # Build triplet\n",
    "                if inverse:\n",
    "                    triplets.append((target, rel, curr))\n",
    "                else:\n",
    "                    triplets.append((curr, rel, target))\n",
    "\n",
    "                # move pointer\n",
    "                curr = target\n",
    "\n",
    "    # Deduplicate final results\n",
    "    triplets = list(dict.fromkeys(triplets))\n",
    "    sample[\"triplet\"] = triplets\n",
    "    return sample\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "def linearize(triplets: List[Tuple[str, str, str]]) -> str:\n",
    "    return \"\\n\".join(f\"<e>{h}</e> || {r} || <e>{t}</e>\" for h, r, t in triplets)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(data: dict, remove_underscore: bool = True) -> Tuple[Dict, List]:\n",
    "    from tqdm import tqdm\n",
    "    \"\"\"\n",
    "    Create triplets from given FactKG structure.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Input data containing 'Entity_set' and 'Evidence'.\n",
    "    - remove_underscore (bool): If True, replace underscores with spaces in entity names.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[Dict, List]: A tuple containing the updated data dictionary and the list distinct entity used for later update the trie.\n",
    "    \"\"\"\n",
    "    updated_data = {}\n",
    "    distinct_entities = set()\n",
    "    keys = list(data.keys())\n",
    "    for key in tqdm(keys, desc=\"Processing data\"):\n",
    "        updated = generate(key, data[key])\n",
    "        updated_data[key] = updated\n",
    "\n",
    "        # Collect distinct entities from all triplets\n",
    "        for triplet in updated[\"triplet\"]:\n",
    "            # Triplet contains 3 elements, get the first one and the last one as entities\n",
    "            distinct_entities.add(triplet[0])\n",
    "            distinct_entities.add(triplet[2])\n",
    "\n",
    "    return updated_data, list(distinct_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fa3199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: d:\\claimpkg\\claimpkg-clone\\src\\notebooks\\..\\resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 5000/5000 [00:00<00:00, 61507.81it/s]\n",
      "Processing data: 100%|██████████| 1000/1000 [00:00<00:00, 132529.83it/s]\n",
      "Processing data: 100%|██████████| 300/300 [00:00<00:00, 85481.74it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'resources'\n",
    "# Data dir = (1) working directory, (2) move out of test, (3) move out of src, and append to resources\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'resources')\n",
    "print(\"Data Directory:\", DATA_DIR)\n",
    "\n",
    "TRAIN_FILE = 'factkg_train_5k.pickle'\n",
    "TEST_FILE = 'factkg_test_1k.pickle'\n",
    "VALID_FILE = 'factkg_val_300.pickle'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "VALID_FILE_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
    "\n",
    "import pickle\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "valid_data = None\n",
    "with open(TRAIN_FILE_PATH, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(TEST_FILE_PATH, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open(VALID_FILE_PATH, 'rb') as f:\n",
    "    valid_data = pickle.load(f)\n",
    "\n",
    "train_updated_data, train_distinct_entities = process_data(train_data, remove_underscore=True)\n",
    "\n",
    "test_updated_data, test_distinct_entities = process_data(test_data, remove_underscore=True)\n",
    "\n",
    "valid_updated_data, valid_distinct_entities = process_data(valid_data, remove_underscore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5909f4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Label': [True],\n",
       " 'Entity_set': ['\"2003-09-27\"', 'Arianespace', 'Guiana_Space_Centre'],\n",
       " 'Evidence': {'Arianespace': [['~manufacturer', 'sites'],\n",
       "   ['~manufacturer', 'finalFlight']],\n",
       "  'Guiana_Space_Centre': [['~sites', 'manufacturer'],\n",
       "   ['~sites', 'finalFlight']],\n",
       "  '\"2003-09-27\"': [['~finalFlight', 'manufacturer'],\n",
       "   ['~finalFlight', 'sites']]},\n",
       " 'types': ['written', 'num3', 'multi hop'],\n",
       " 'triplet': [('unknown_1', 'manufacturer', 'unknown_0'),\n",
       "  ('unknown_1', 'sites', 'Guiana Space Centre'),\n",
       "  ('unknown_2', 'manufacturer', 'unknown_0'),\n",
       "  ('unknown_2', 'finalFlight', '\"2003-09-27\"'),\n",
       "  ('unknown_3', 'sites', 'unknown_0'),\n",
       "  ('unknown_3', 'manufacturer', 'Arianespace'),\n",
       "  ('unknown_4', 'sites', 'unknown_0'),\n",
       "  ('unknown_4', 'finalFlight', '\"2003-09-27\"'),\n",
       "  ('unknown_5', 'finalFlight', 'unknown_0'),\n",
       "  ('unknown_5', 'manufacturer', 'Arianespace'),\n",
       "  ('unknown_6', 'finalFlight', 'unknown_0'),\n",
       "  ('unknown_6', 'sites', 'Guiana Space Centre')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_updated_data['Guiana Space Centre was the launch site of the rocket which was manufactured by Arianespace and had its final flight on 27th September, 2003.']\n",
    "\n",
    "#Guiana Space Centre was the launch site of the rocket which was manufactured by Arianespace and had its final flight on 27th September, 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe1e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the processed data back to pickle files\n",
    "with open(os.path.join(DATA_DIR, 'factkg_train_5k_triplets.pickle'), 'wb') as f:\n",
    "    pickle.dump(train_updated_data, f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'factkg_test_1k_triplets.pickle'), 'wb') as f:\n",
    "    pickle.dump(test_updated_data, f)\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'factkg_dev_300_triplets.pickle'), 'wb') as f:\n",
    "    pickle.dump(valid_updated_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da95a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct entities across all datasets: 5269\n"
     ]
    }
   ],
   "source": [
    "all_join_entities = set()\n",
    "all_join_entities.update(train_distinct_entities)\n",
    "all_join_entities.update(test_distinct_entities)\n",
    "all_join_entities.update(valid_distinct_entities)\n",
    "\n",
    "print(\"Total distinct entities across all datasets:\", len(all_join_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0518cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as a pickle file contains a set, later use this for a trie reformation\n",
    "with open(os.path.join(DATA_DIR, 'factkg_all_distinct_entities.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_join_entities, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
