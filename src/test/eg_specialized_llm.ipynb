{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085371d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pip install transformers==4.44.2\n",
    "pip install accelerate==0.33.0\n",
    "pip install peft==0.14.0\n",
    "pip install datasets\n",
    "pip install sentencepiece\n",
    "pip install bitsandbytes\n",
    "pip install hf-xet==1.2.0\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "(.venv) PS D:\\claimpkg\\claimpkg-clone> huggingface-cli login\n",
    "‚ö†Ô∏è  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth loogin' instead.\n",
    "\n",
    "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
    "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
    "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
    "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
    "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
    "\n",
    "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
    "Token can be pasted using 'Right-Click'.\n",
    "Enter your token (input will not be visible):\n",
    "Add token as git credential? (Y/n) Y\n",
    "Token is valid (permission: fineGrained).\n",
    "The token `Llama-3.2-1B-TNG-token` has been saved to C:\\Users\\tungq\\.cache\\huggingface\\stored_tokens\n",
    "Your token has been saved in your configured git credential helpers (manager).\n",
    "Your token has been saved to C:\\Users\\tungq\\.cache\\huggingface\\token\n",
    "Login successful.\n",
    "The current active token is: `Llama-3.2-1B-TNG-token`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8622114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5cf0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: Barack Obama was born in Hawaii. It's a lie.\n",
      "By: The Daily Caller | August 25, 2012\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "# Check if model is runable now\n",
    "\n",
    "text = \"Claim: Barack Obama was born in Hawaii.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acda9050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ad063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "DATA_DIR = 'resources'\n",
    "# Data dir = (1) working directory, (2) move out of test, (3) move out of src, and append to resources\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'resources')\n",
    "\n",
    "TRAIN_FILE = 'finetune_train_data.pickle'\n",
    "TEST_FILE = 'finetune_test_data.pickle'\n",
    "VALID_FILE = 'finetune_validation_data.pickle'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "VALID_FILE_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "valid_data = None\n",
    "\n",
    "# Load\n",
    "with open(TRAIN_FILE_PATH, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(TEST_FILE_PATH, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open(VALID_FILE_PATH, 'rb') as f:\n",
    "    valid_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ec5892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528bb9312e98486d960b74c5d66fe185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2f723fda804428aed2e8e4dc6d25e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad9f0473f4c4a2e819c15142bab301e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_list(train_data)\n",
    "val_ds   = Dataset.from_list(valid_data)\n",
    "test_ds  = Dataset.from_list(test_data)\n",
    "\n",
    "def format_fn(example):\n",
    "    example[\"prompt\"] = f\"Claim: {example['input']}\\nGenerate pseudo-subgraph:\\n\"\n",
    "    example[\"labels\"] = example[\"output\"]\n",
    "    return example\n",
    "\n",
    "train_ds = train_ds.map(format_fn)\n",
    "val_ds   = val_ds.map(format_fn)\n",
    "test_ds  = test_ds.map(format_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba3b918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aae603eac594b1abd14b3ba40045dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a78dd7699c410382854b9079789c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(batch):\n",
    "    \"\"\"\n",
    "    Tokenize prompts and labels for causal LM fine-tuning.\n",
    "    For causal LM, we concatenate prompt + labels and use them as input_ids.\n",
    "    Labels are the same as input_ids, but we mask the prompt part with -100\n",
    "    so loss is only computed on the generated output tokens.\n",
    "    \"\"\"\n",
    "    # Concatenate prompt and labels to form the full text\n",
    "    full_texts = [\n",
    "        prompt + label\n",
    "        for prompt, label in zip(batch[\"prompt\"], batch[\"labels\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize prompts separately to know where to mask\n",
    "    prompt_lengths = []\n",
    "    for prompt in batch[\"prompt\"]:\n",
    "        prompt_tok = tokenizer(prompt, truncation=False, padding=False)\n",
    "        prompt_lengths.append(len(prompt_tok[\"input_ids\"]))\n",
    "\n",
    "    # Tokenize full text with padding and truncation enabled\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # pad to max_length for uniform batches\n",
    "        max_length=512,  # reduced from 1024 to avoid memory issues; adjust as needed\n",
    "        return_tensors=None,  # return lists, Trainer will convert to tensors\n",
    "    )\n",
    "\n",
    "    # Create labels: copy input_ids and mask prompt tokens with -100\n",
    "    labels = []\n",
    "    for i, input_ids in enumerate(tokenized[\"input_ids\"]):\n",
    "        label = input_ids.copy()\n",
    "        # Mask prompt tokens (set to -100 so they're ignored in loss)\n",
    "        prompt_len = min(prompt_lengths[i], len(label))\n",
    "        for j in range(prompt_len):\n",
    "            label[j] = -100\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\claimpkg\\claimpkg-clone\\.venv\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a1dd83df014fc085c98097f52dad37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\claimpkg\\claimpkg-clone\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423ee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.44.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "\n",
    "transformers.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
