{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fe90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = r'D:\\claimpkg\\claimpkg-clone\\src\\resources\\model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fece63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\claimpkg\\claimpkg-clone\\.venv\\lib\\site-packages\\peft\\config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'qalora_group_size', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_qalora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "d:\\claimpkg\\claimpkg-clone\\.venv\\lib\\site-packages\\peft\\config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'qalora_group_size', 'trainable_token_indices', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the PEFT config\n",
    "peft_config = PeftConfig.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Load the base model (same as used during training)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# Load the adapter weights on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_DIR)\n",
    "\n",
    "# Load tokenizer\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f2178b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: d:\\claimpkg\\claimpkg-clone\\src\\test\\..\\resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "DATA_DIR = 'resources'\n",
    "# Data dir = (1) working directory, (2) move out of test, (3) move out of src, and append to resources\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'resources')\n",
    "print(\"Data Directory:\", DATA_DIR)\n",
    "\n",
    "TRAIN_FILE = 'finetune_train_data.pickle'\n",
    "TEST_FILE = 'finetune_test_data.pickle'\n",
    "VALID_FILE = 'finetune_validation_data.pickle'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "VALID_FILE_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "valid_data = None\n",
    "\n",
    "import pickle\n",
    "# Load\n",
    "with open(TRAIN_FILE_PATH, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(TEST_FILE_PATH, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open(VALID_FILE_PATH, 'rb') as f:\n",
    "    valid_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c902ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Claim: Khalid Mahmood is the leader of a city which was the birthplace of architect, Vedat Tek, who designed 103 Colmore Row and I.C.Tower.\n",
      "Generate pseudo-subgraph:\n",
      "\n",
      "\n",
      "Generated Output Text:\n",
      " Claim: Khalid Mahmood is the leader of a city which was the birthplace of architect, Vedat Tek, who designed 103 Colmore Row and I.C.Tower.\n",
      "Generate pseudo-subgraph:\n",
      "<e>103 Colmore Row</e> || architect || <e>Vedat Tek</e>\n",
      "<e>103 Colmore Row</e> || architect || <e>Khalid Mahmood</e>\n",
      "<e>103 Colmore Row</e> || architect || <e>I.C.Tower</e>\n",
      "<e>Vedat Tek</e> || birthPlace || <e>103 Colmore Row</e>\n",
      "<e>Vedat Tek</e> || birthPlace || <e>Khalid Mahmood</e>\n",
      "<e>Vedat Tek</e> || birthPlace || <e>I.C.Tower</e>\n",
      "<e>103 Colmore Row</e> || architect || <e>Vedat Tek</e>\n",
      "<e>Khalid Mahmood</e> || architect || <e>Vedat Tek</e>\n",
      "<e>I.C.Tower</e> || architect || <e>Vedat Tek</e>\n",
      "<e>Vedat Tek</e> || leader || <e>103 Colmore Row</e>\n",
      "<e>Vedat Tek</e> || leader || <e>Khalid Mahmood</e>\n",
      "<e>Vedat Tek\n"
     ]
    }
   ],
   "source": [
    "# Run validation with item 5\n",
    "item = 'Khalid Mahmood is the leader of a city which was the birthplace of architect, Vedat Tek, who designed 103 Colmore Row and I.C.Tower.'\n",
    "input_text = f\"Claim: {item}\\nGenerate pseudo-subgraph:\\n\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "output_ids = model.generate(input_ids, max_new_tokens=250)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Input Text:\\n\", input_text)\n",
    "# print(\"Ground Truth Output Text:\\n\", item['output'])\n",
    "print(\"\\nGenerated Output Text:\\n\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d99e9e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Claim: Tay Garnett is the director of English without Tears.\n",
      "Generate pseudo-subgraph:\n",
      "\n",
      "Ground Truth Output Text:\n",
      " <e>English Without Tears</e> || director || <e>Tay Garnett</e>\n",
      "<e>English Without Tears</e> || director || <e>Tay Garnett</e>\n",
      "\n",
      "Generated Output Text:\n",
      " Claim: Tay Garnett is the director of English without Tears.\n",
      "Generate pseudo-subgraph:\n",
      "  1. Get all the actors in the database\n",
      "  2. Get all the movies they starred in\n",
      "  3. Get all the movies they acted in\n",
      "  4. Get all the movies they acted in\n",
      "  5. Get all the movies they acted in\n",
      "  6. Get all the movies they acted in\n",
      "  7. Get all the movies they acted in\n",
      "  8. Get all the movies they acted in\n",
      "  9. Get all the movies they acted in\n",
      "  10. Get all the movies they acted in\n",
      "  11. Get all the movies they acted in\n",
      "  12. Get all the movies they acted in\n",
      "  13. Get all the movies they acted in\n",
      "  14. Get all the movies they acted in\n",
      "  15. Get all the movies they acted in\n",
      "  16. Get all the movies they acted in\n",
      "  17. Get all the movies\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model_not_ft = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model_not_ft.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Check if model is runable now\n",
    "item = valid_data[50]\n",
    "input_text = f\"Claim: {item['input']}\\nGenerate pseudo-subgraph:\\n\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "output_ids = model_not_ft.generate(input_ids, max_new_tokens=200)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Input Text:\\n\", input_text)\n",
    "print(\"Ground Truth Output Text:\\n\", item['output'])\n",
    "print(\"\\nGenerated Output Text:\\n\", output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
