{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78534a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils.dataset_processing import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4860e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "remove_underscore = True\n",
    "\n",
    "def _norm(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    return s.replace(\"_\", \" \") if remove_underscore else s\n",
    "\n",
    "def _new_unknown(unk_list):\n",
    "    u = f\"unknown_{len(unk_list)}\"\n",
    "    unk_list.append(u)\n",
    "    return u\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#          →→  CORRECT GRAPH CONSTRUCTION FOR FACTKG  ←←\n",
    "# ============================================================\n",
    "def generate(claim: str, sample: Dict[str, Any]):\n",
    "    entity_set = [_norm(e) for e in sample.get(\"Entity_set\", [])]\n",
    "    evidence = sample.get(\"Evidence\", {})\n",
    "\n",
    "    unknown_list = []\n",
    "    triplets = []\n",
    "\n",
    "    # Mapping raw → normalized\n",
    "    evidence_map = { _norm(k): v for k, v in evidence.items() }\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # For each entity and each evidence relation group\n",
    "    # ----------------------------------------------------------\n",
    "    for ent_norm, rel_groups in evidence_map.items():\n",
    "\n",
    "        for rel_path in rel_groups:\n",
    "\n",
    "            # ----------------------------------------------------------\n",
    "            # CASE 1 — Single hop (no unknown). We must find real target.\n",
    "            # ----------------------------------------------------------\n",
    "            if len(rel_path) == 1:\n",
    "                r = rel_path[0]\n",
    "                inverse = r.startswith(\"~\")\n",
    "                rel = r[1:] if inverse else r\n",
    "\n",
    "                target_entity = None\n",
    "\n",
    "                # Search for target: entity that has inverse relation\n",
    "                if not inverse:\n",
    "                    # forward relation\n",
    "                    # find entity E with \"~rel\"\n",
    "                    for cand, groups in evidence_map.items():\n",
    "                        for g in groups:\n",
    "                            if f\"~{rel}\" in g:\n",
    "                                target_entity = cand\n",
    "                                break\n",
    "                        if target_entity:\n",
    "                            break\n",
    "                else:\n",
    "                    # inverse relation\n",
    "                    # find entity E with forward rel\n",
    "                    for cand, groups in evidence_map.items():\n",
    "                        for g in groups:\n",
    "                            if rel in g:      # NOT \"~rel\"\n",
    "                                target_entity = cand\n",
    "                                break\n",
    "                        if target_entity:\n",
    "                            break\n",
    "\n",
    "                # If cannot find target (rare), fallback to unknown\n",
    "                if target_entity is None:\n",
    "                    target_entity = _new_unknown(unknown_list)\n",
    "\n",
    "                # Construct triplet\n",
    "                if inverse:\n",
    "                    triplets.append((target_entity, rel, ent_norm))\n",
    "                else:\n",
    "                    triplets.append((ent_norm, rel, target_entity))\n",
    "\n",
    "                continue\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # CASE 2 — Multi-hop relation path\n",
    "            # ent_norm is starting point\n",
    "            # --------------------------------------------------\n",
    "            prev = ent_norm\n",
    "\n",
    "            for r in rel_path:\n",
    "                inverse = r.startswith(\"~\")\n",
    "                rel = r[1:] if inverse else r\n",
    "\n",
    "                next_node = _new_unknown(unknown_list)\n",
    "\n",
    "                if inverse:\n",
    "                    triplets.append((next_node, rel, prev))\n",
    "                else:\n",
    "                    triplets.append((prev, rel, next_node))\n",
    "\n",
    "                prev = next_node\n",
    "\n",
    "    # Deduplicate\n",
    "    triplets = list(dict.fromkeys(triplets))\n",
    "    sample[\"triplet\"] = triplets\n",
    "    return sample\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "def linearize(triplets: List[Tuple[str, str, str]]) -> str:\n",
    "    return \"\\n\".join(f\"<e>{h}</e> || {r} || <e>{t}</e>\" for h, r, t in triplets)\n",
    "\n",
    "def process_data(data: dict, remove_underscore: bool = True) -> Tuple[Dict, List]:\n",
    "    from tqdm import tqdm\n",
    "    \"\"\"\n",
    "    Create triplets from given FactKG structure.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Input data containing 'Entity_set' and 'Evidence'.\n",
    "    - remove_underscore (bool): If True, replace underscores with spaces in entity names.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[Dict, List]: A tuple containing the updated data dictionary and the list distinct entity used for later update the trie.\n",
    "    \"\"\"\n",
    "    updated_data = {}\n",
    "    distinct_entities = set()\n",
    "    keys = list(data.keys())\n",
    "    for key in tqdm(keys, desc=\"Processing data\"):\n",
    "        updated = generate(key, data[key])\n",
    "        updated_data[key] = updated\n",
    "\n",
    "        # Collect distinct entities from all triplets\n",
    "        for triplet in updated[\"triplet\"]:\n",
    "            # Triplet contains 3 elements, get the first one and the last one as entities\n",
    "            distinct_entities.add(triplet[0])\n",
    "            distinct_entities.add(triplet[2])\n",
    "\n",
    "    return updated_data, list(distinct_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880509db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: d:\\claimpkg\\claimpkg-clone\\src\\test\\..\\resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 300/300 [00:00<00:00, 149458.51it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'resources'\n",
    "# Data dir = (1) working directory, (2) move out of test, (3) move out of src, and append to resources\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'resources')\n",
    "print(\"Data Directory:\", DATA_DIR)\n",
    "\n",
    "TRAIN_FILE = 'factkg_train_5k.pickle'\n",
    "TEST_FILE = 'factkg_test_1k.pickle'\n",
    "VALID_FILE = 'factkg_val_300.pickle'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR, TRAIN_FILE)\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR, TEST_FILE)\n",
    "VALID_FILE_PATH = os.path.join(DATA_DIR, VALID_FILE)\n",
    "\n",
    "import pickle\n",
    "\n",
    "valid_data = None\n",
    "with open(VALID_FILE_PATH, 'rb') as f:\n",
    "    valid_data = pickle.load(f)\n",
    "\n",
    "valid_updated_data, valid_distinct_entities = process_data(valid_data, remove_underscore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d26f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.pseudograph_checking_llm import PseudoGraphCheckingLLM\n",
    "\n",
    "keys = list(valid_updated_data.keys())[:10]\n",
    "\n",
    "res = []\n",
    "pseudograph_llm = PseudoGraphCheckingLLM()\n",
    "for key in keys:\n",
    "    sample = valid_updated_data[key]\n",
    "    triplets = sample[\"triplet\"]\n",
    "    linearized_triplets = linearize(triplets)\n",
    "\n",
    "    response = pseudograph_llm.submit(sample, linearized_triplets)\n",
    "    res.append((response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4f940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8dae1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CORRECT', 'CORRECT', 'CORRECT', 'CORRECT', 'CORRECT', 'CORRECT', 'CORRECT', 'CORRECT', 'CORRECT', 'CORRECT']\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "befe23ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guiana Space Centre was the launch site of the rocket which was manufactured by Arianespace and had its final flight on 27th September, 2003.\n",
      "{'Label': [True], 'Entity_set': ['\"2003-09-27\"', 'Arianespace', 'Guiana_Space_Centre'], 'Evidence': {'Arianespace': [['~manufacturer', 'sites'], ['~manufacturer', 'finalFlight']], 'Guiana_Space_Centre': [['~sites', 'manufacturer'], ['~sites', 'finalFlight']], '\"2003-09-27\"': [['~finalFlight', 'manufacturer'], ['~finalFlight', 'sites']]}, 'types': ['written', 'num3', 'multi hop'], 'triplet': [('unknown_0', 'manufacturer', 'Arianespace'), ('unknown_0', 'sites', 'unknown_1'), ('unknown_2', 'manufacturer', 'Arianespace'), ('unknown_2', 'finalFlight', 'unknown_3'), ('unknown_4', 'sites', 'Guiana Space Centre'), ('unknown_4', 'manufacturer', 'unknown_5'), ('unknown_6', 'sites', 'Guiana Space Centre'), ('unknown_6', 'finalFlight', 'unknown_7'), ('unknown_8', 'finalFlight', '\"2003-09-27\"'), ('unknown_8', 'manufacturer', 'unknown_9'), ('unknown_10', 'finalFlight', '\"2003-09-27\"'), ('unknown_10', 'sites', 'unknown_11')]}\n"
     ]
    }
   ],
   "source": [
    "INDEX = 3\n",
    "keys = list(valid_updated_data.keys())\n",
    "print(keys[INDEX])\n",
    "print(valid_updated_data[keys[INDEX]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
